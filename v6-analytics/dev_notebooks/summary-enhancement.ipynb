{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from vantage6.algorithm.tools.util import info\n",
    "from vantage6.algorithm.tools.exceptions import AlgorithmExecutionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary disable some privacy settings that are defined in the v6-summary-py\n",
    "_summary = import_module(\"v6-summary-py\")\n",
    "_summary.utils.DEFAULT_MINIMUM_ROWS = 0\n",
    "_summary.utils.DEFAULT_PRIVACY_THRESHOLD = 0\n",
    "\n",
    "_summary.partial_summary.DEFAULT_MINIMUM_ROWS = 0\n",
    "_summary.partial_summary.DEFAULT_PRIVACY_THRESHOLD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data\n",
    "df1 = pd.read_csv('/Users/vramella/Library/CloudStorage/OneDrive-BIOMERIS/PROGETTI-CLIENTI/RC-INT/KWF - Blueberry/Vantage6/Algoritmi-dev/summary-mod//data_org1.csv')\n",
    "df2 = pd.read_csv('/Users/vramella/Library/CloudStorage/OneDrive-BIOMERIS/PROGETTI-CLIENTI/RC-INT/KWF - Blueberry/Vantage6/Algoritmi-dev/summary-mod//data_org2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_cohorts(df, cohort_1_fraction=0.6):\n",
    "    mask = np.random.rand(len(df)) < cohort_1_fraction\n",
    "    cohort_1 = df[mask].copy()\n",
    "    cohort_2 = df[~mask].copy()\n",
    "    return cohort_1, cohort_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organization 1\n",
    "sarcoma_cohort_1, sarcoma_cohort_2 = split_into_cohorts(df1)\n",
    "org1 = {\"cohort_1\": sarcoma_cohort_1, \"cohort_2\": sarcoma_cohort_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organization 2\n",
    "sarcoma_cohort_1, sarcoma_cohort_2 = split_into_cohorts(df2)\n",
    "org2 = {\"cohort_1\": sarcoma_cohort_1, \"cohort_2\": sarcoma_cohort_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to compute the summary statistics for\n",
    "columns = ['Sex', 'age', 'Height', 'Weight']\n",
    "numeric_columns = ['age', 'Height', 'Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _aggregate_partial_summaries(results: list[dict]) -> dict:\n",
    "    \"\"\"Aggregate the partial summaries of all nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : list[dict]\n",
    "        The partial summaries of all nodes.\n",
    "    \"\"\"\n",
    "    info(\"Aggregating partial summaries\")\n",
    "    aggregated_summary = {}\n",
    "    is_first = True\n",
    "    for result in results:\n",
    "        if result is None:\n",
    "            raise AlgorithmExecutionError(\n",
    "                \"At least one of the nodes returned invalid result. Please check the \"\n",
    "                \"logs.\"\n",
    "            )\n",
    "        if is_first:\n",
    "            # copy results. Only convert num complete rows per node to a list so that\n",
    "            # we can add the other nodes to it later\n",
    "            aggregated_summary = result\n",
    "            aggregated_summary[\"num_complete_rows_per_node\"] = [\n",
    "                result[\"num_complete_rows_per_node\"]\n",
    "            ]\n",
    "            for column in result[\"numeric\"]:\n",
    "                aggregated_summary[\"numeric\"][column][\"median\"] = [\n",
    "                    result[\"numeric\"][column][\"median\"]\n",
    "                ]\n",
    "                aggregated_summary[\"numeric\"][column][\"q_25\"] = [\n",
    "                    result[\"numeric\"][column][\"q_25\"]\n",
    "                ]\n",
    "                aggregated_summary[\"numeric\"][column][\"q_75\"] = [\n",
    "                    result[\"numeric\"][column][\"q_75\"]\n",
    "                ]\n",
    "            is_first = False\n",
    "            continue\n",
    "\n",
    "        # aggregate data for numeric columns\n",
    "        for column in result[\"numeric\"]:\n",
    "            aggregated_dict = aggregated_summary[\"numeric\"][column]\n",
    "            aggregated_dict[\"count\"] += result[\"numeric\"][column][\"count\"]\n",
    "            aggregated_dict[\"min\"] = min(\n",
    "                aggregated_summary[\"numeric\"][column][\"min\"],\n",
    "                result[\"numeric\"][column][\"min\"],\n",
    "            )\n",
    "            aggregated_dict[\"max\"] = max(\n",
    "                aggregated_summary[\"numeric\"][column][\"max\"],\n",
    "                result[\"numeric\"][column][\"max\"],\n",
    "            )\n",
    "            aggregated_dict[\"missing\"] += result[\"numeric\"][column][\"missing\"]\n",
    "            aggregated_dict[\"sum\"] += result[\"numeric\"][column][\"sum\"]\n",
    "            aggregated_dict[\"median\"].append(result[\"numeric\"][column][\"median\"])\n",
    "            aggregated_dict[\"q_25\"].append(result[\"numeric\"][column][\"q_25\"])\n",
    "            aggregated_dict[\"q_75\"].append(result[\"numeric\"][column][\"q_75\"])\n",
    "\n",
    "        # aggregate data for categorical columns\n",
    "        for column in result[\"categorical\"]:\n",
    "            aggregated_dict = aggregated_summary[\"categorical\"][column]\n",
    "            aggregated_dict[\"count\"] += result[\"categorical\"][column][\"count\"]\n",
    "            aggregated_dict[\"missing\"] += result[\"categorical\"][column][\"missing\"]\n",
    "\n",
    "        # add the number of complete rows for this node\n",
    "        aggregated_summary[\"num_complete_rows_per_node\"].append(\n",
    "            result[\"num_complete_rows_per_node\"]\n",
    "        )\n",
    "\n",
    "        # add the unique values\n",
    "        for column in result[\"counts_unique_values\"]:\n",
    "            if column not in aggregated_summary[\"counts_unique_values\"]:\n",
    "                aggregated_summary[\"counts_unique_values\"][column] = {}\n",
    "            for value, count in result[\"counts_unique_values\"][column].items():\n",
    "                if value not in aggregated_summary[\"counts_unique_values\"][column]:\n",
    "                    aggregated_summary[\"counts_unique_values\"][column][value] = 0\n",
    "                aggregated_summary[\"counts_unique_values\"][column][value] += count\n",
    "\n",
    "    # now that all data is aggregated, we can compute the mean\n",
    "    for column in aggregated_summary[\"numeric\"]:\n",
    "        aggregated_dict = aggregated_summary[\"numeric\"][column]\n",
    "        if aggregated_dict[\"count\"]:\n",
    "            aggregated_dict[\"mean\"] = aggregated_dict[\"sum\"] / aggregated_dict[\"count\"]\n",
    "        else:\n",
    "            aggregated_dict[\"mean\"] = 0  # TODO this is terrible, we should not do this\n",
    "\n",
    "    return aggregated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_sd_to_results(\n",
    "    results: dict, variance_results: list[dict], numerical_columns: list[str]\n",
    ") -> dict:\n",
    "    \"\"\"Add the variance to the results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        The results of the summary task.\n",
    "    variance_results : list[dict]\n",
    "        The variance results of all nodes.\n",
    "    numerical_columns : list[str]\n",
    "        The numerical columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The results with the variance added.\n",
    "    \"\"\"\n",
    "    for column in numerical_columns:\n",
    "        sum_variance = 0\n",
    "        for node_results in variance_results:\n",
    "            sum_variance += node_results[column]\n",
    "        if results[\"numeric\"][column][\"count\"] > 1:\n",
    "            variance = sum_variance / (results[\"numeric\"][column][\"count\"] - 1)\n",
    "        else:\n",
    "            variance = 0  # TODO THIS IS TERRIBLE\n",
    "        results[\"numeric\"][column][\"std\"] = variance**0.5\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_per_data_station(\n",
    "    dfs: list[pd.DataFrame], cohort_names: list[str], *args, **kwargs\n",
    ") -> dict:\n",
    "    results = {}\n",
    "    for df, name in zip(dfs, cohort_names):\n",
    "        results[name] = _summary.partial_summary._summary_per_data_station(\n",
    "            df, *args, **kwargs\n",
    "        )\n",
    "        # Add median and quantiles (0.25, 0.75)\n",
    "        for var in results[name]['numeric']:\n",
    "            results[name]['numeric'][var]['median'] = float(np.nanmedian(df[var]))\n",
    "            results[name]['numeric'][var]['q_25'] = float(np.nanquantile(df[var], 0.25))\n",
    "            results[name]['numeric'][var]['q_75'] = float(np.nanquantile(df[var], 0.75))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_per_data_station(\n",
    "    dfs: list[pd.DataFrame],\n",
    "    cohort_names: list[str],\n",
    "    means: dict[list[float]],\n",
    "    *args,\n",
    "    **kwargs,\n",
    ") -> dict:\n",
    "    results = {}\n",
    "    info(kwargs)\n",
    "    info(means)\n",
    "    for df, name in zip(dfs, cohort_names):\n",
    "        info(\"*\" * 80)\n",
    "        info(name)\n",
    "        info(means[name])\n",
    "        info(\"*\" * 80)\n",
    "        results[name] = _summary.partial_variance._variance_per_data_station(\n",
    "            df, means=means[name], *args, **kwargs\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = summary_per_data_station(dfs=list(org1.values()), cohort_names=list(org1.keys()), columns=columns, numeric_columns=numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = summary_per_data_station(dfs=list(org2.values()), cohort_names=list(org2.keys()), columns=columns, numeric_columns=numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [res1, res2]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the partial summaries of all nodes\n",
    "all_cohort_results = {}\n",
    "\n",
    "means = {}\n",
    "cohort_names = results[0].keys()\n",
    "print(cohort_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort_name in cohort_names:\n",
    "    print(cohort_name)\n",
    "    for result in results:\n",
    "        print(result[cohort_name])\n",
    "    cohort_results = [result[cohort_name] for result in results]\n",
    "    all_cohort_results[cohort_name] = _aggregate_partial_summaries(cohort_results)\n",
    "\n",
    "    numerical_columns = list(all_cohort_results[cohort_name][\"numeric\"].keys())\n",
    "    # compute the variance now that we have the mean\n",
    "    means[cohort_name] = [\n",
    "        all_cohort_results[cohort_name][\"numeric\"][column][\"mean\"]\n",
    "        for column in numerical_columns\n",
    "    ]\n",
    "    info(f\"n num cols: {len(numerical_columns)}\")\n",
    "    info(f\"n means: {len(means[cohort_name])}\")\n",
    "\n",
    "    info(\"debugger\")\n",
    "    info(numerical_columns)\n",
    "    info(len(means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cohort_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance per data station\n",
    "var1 = variance_per_data_station(dfs=list(org1.values()), cohort_names=list(org1.keys()), means=means, columns=numerical_columns)\n",
    "var2 = variance_per_data_station(dfs=list(org2.values()), cohort_names=list(org2.keys()), means=means, columns=numerical_columns)\n",
    "variance_results = [var1, var2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort_name in cohort_names:\n",
    "        cohort_variance_results = [result[cohort_name] for result in variance_results]\n",
    "        all_cohort_results[cohort_name] = _add_sd_to_results(\n",
    "            all_cohort_results[cohort_name], cohort_variance_results, numerical_columns\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cohort_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-summary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
